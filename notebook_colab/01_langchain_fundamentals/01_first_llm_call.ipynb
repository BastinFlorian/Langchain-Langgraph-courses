{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-google-genai python-dotenv langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First LLM Call with Google Gemini and LangChain\n",
    "\n",
    "In this notebook, we learn:\n",
    "- Setting up Google Gemini API keys\n",
    "- Initializing a LangChain chat model\n",
    "- Making first simple call with invoke()\n",
    "- Comparing different Gemini models\n",
    "\n",
    "Official documentation:\n",
    "- LangChain: https://python.langchain.com/docs/\n",
    "- Google Gemini: https://ai.google.dev/gemini-api/docs/\n",
    "- LangChain Google GenAI: https://python.langchain.com/docs/integrations/chat/google_generative_ai/\n",
    "\n",
    "Your task:\n",
    "- Execute the cells step by step\n",
    "- Understand each part of the code and how to call a LLM\n",
    "- Debug the empty answer by understanding the cause and fixing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration detected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API key is configured\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  GOOGLE_API_KEY is not configured in .env file\")\n",
    "    print(\"Please add your Google Gemini API key to the .env file:\")\n",
    "    print(\"GOOGLE_API_KEY=your_api_key_here\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"‚úÖ Configuration detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INITIALIZE GEMINI 2.5 FLASH MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñ Initializing Gemini 2.5 Flash model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model models/gemini-2.5-flash initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with basic parameters\n",
    "llm_flash = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",  # Latest and fastest model\n",
    "    # Controls creativity (0 = deterministic, 1 = creative)\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,          # Response length limit\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model {llm_flash.model} initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FIRST SIMPLE CALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù First call with a simple question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Response: Hello! I am a large language model, trained by Google.\n",
      "üìä Metadata: {'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Direct call with a string\n",
    "response = llm_flash.invoke(\n",
    "    \"Hello! Can you introduce yourself in one sentence?\")\n",
    "\n",
    "print(f\"ü§ñ Response: {response.content}\")\n",
    "print(f\"üìä Metadata: {response.response_metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. COMPARISON WITH GEMINI 2.0 FLASH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîÑ Comparison with Gemini 2.0 Flash..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Question: Explain the difference between artificial intelligence and machine learning in one simple sentence.\n",
      "\n",
      "üü¶ Gemini 2.5 Flash: \n",
      "\n",
      "üü© Gemini 2.0 Flash: Artificial intelligence is the broader concept of machines mimicking human intelligence, while machine learning is a specific approach to achieving AI by allowing machines to learn from data without explicit programming.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini 2.0 model\n",
    "llm_2_0 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# More complex question to see the difference\n",
    "question = \"Explain the difference between artificial intelligence and machine learning in one simple sentence.\"\n",
    "\n",
    "print(f\"\\n‚ùì Question: {question}\")\n",
    "\n",
    "# Response with Gemini 2.5\n",
    "response_2_5 = llm_flash.invoke(question)\n",
    "print(f\"\\nüü¶ Gemini 2.5 Flash: {response_2_5.content}\")\n",
    "\n",
    "# Response with Gemini 2.0\n",
    "response_2_0 = llm_2_0.invoke(question)\n",
    "print(f\"\\nüü© Gemini 2.0 Flash: {response_2_0.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PARAMETER TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚öôÔ∏è  Testing with different parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Creative question: Tell me a short and original story about a robot learning to cook.\n",
      "\n",
      "üé® Creative mode (temp=1.0): \n",
      "\n",
      "üéØ Precise mode (temp=0.0): \n"
     ]
    }
   ],
   "source": [
    "# More creative model\n",
    "llm_creative = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=1.0,      # Maximum creativity\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# More deterministic model\n",
    "llm_precise = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0,      # More consistent responses\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "creative_question = \"Tell me a short and original story about a robot learning to cook.\"\n",
    "\n",
    "print(f\"\\n‚ùì Creative question: {creative_question}\")\n",
    "\n",
    "# Creative response\n",
    "response_creative = llm_creative.invoke(creative_question)\n",
    "print(f\"\\nüé® Creative mode (temp=1.0): {response_creative.content}\")\n",
    "\n",
    "# Precise response\n",
    "response_precise = llm_precise.invoke(creative_question)\n",
    "print(f\"\\nüéØ Precise mode (temp=0.0): {response_precise.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KEY INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìã Important information:\n",
    "- ‚úÖ invoke(): Main method to call an LLM\n",
    "- ‚úÖ temperature: Controls creativity (0.0 to 1.0)\n",
    "- ‚úÖ max_tokens: Limits response length\n",
    "- ‚úÖ response.content: Contains the response text\n",
    "- ‚úÖ response.response_metadata: Contains metadata (tokens used, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù STUDENT TODO EXERCISE\n",
    "\n",
    "TODO: Create and test your own LLM model configurations\n",
    "\n",
    "Requirements:\n",
    "1. Initialize a Gemini model with these specific parameters:\n",
    "   - model: \"gemini-2.5-flash\"\n",
    "   - temperature: 0.5 (balanced creativity)\n",
    "   - max_tokens: 200\n",
    "\n",
    "2. Test your model with this question:\n",
    "   \"What are 3 benefits of learning programming?\"\n",
    "\n",
    "3. Compare the response with a second model using temperature=0.9\n",
    "\n",
    "Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your first model configuration\n",
    "# my_llm = ChatGoogleGenerativeAI(\n",
    "#     # Add your parameters here\n",
    "# )\n",
    "\n",
    "# TODO: Create your second model with higher temperature\n",
    "# my_creative_llm = ChatGoogleGenerativeAI(\n",
    "#     # Add your parameters here\n",
    "# )\n",
    "\n",
    "# TODO: Test both models with the same question\n",
    "# question = \"What are 3 benefits of learning programming?\"\n",
    "#\n",
    "# response1 = my_llm.invoke(question)\n",
    "# response2 = my_creative_llm.invoke(question)\n",
    "#\n",
    "# print(f\"Balanced response: {response1.content}\")\n",
    "# print(f\"Creative response: {response2.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
