{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-google-genai python-dotenv langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple Chains with LangChain Expression Language (LCEL)\n",
    "======================================================\n",
    "\n",
    "In this notebook, we learn:\n",
    "- Chaining prompt templates with LLMs using the | operator\n",
    "- Understanding LangChain Expression Language (LCEL)\n",
    "- Creating sequential processing workflows\n",
    "- Passing data between chain components\n",
    "\n",
    "Official documentation:\n",
    "- LCEL: https://python.langchain.com/docs/concepts/lcel/\n",
    "- Chains: https://python.langchain.com/docs/how_to/sequence/\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Gemini 2.5 Flash model initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BASIC CHAIN: PROMPT + LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüîó 1. Basic Chain: Prompt + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a simple prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms for a beginner.\"\n",
    ")\n",
    "\n",
    "# Create a chain using the | operator (pipe)\n",
    "basic_chain = prompt | llm\n",
    "\n",
    "# Use the chain\n",
    "result = basic_chain.invoke({\"topic\": \"blockchain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: blockchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: \n"
     ]
    }
   ],
   "source": [
    "print(f\"Output: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CHAIN WITH OUTPUT PARSER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüìù 2. Chain with Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a chain that parses the output to just a string\n",
    "string_parser = StrOutputParser()\n",
    "parsing_chain = prompt | llm | string_parser\n",
    "\n",
    "# Compare outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Without parser ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Content: \n"
     ]
    }
   ],
   "source": [
    "result_without_parser = (prompt | llm).invoke({\"topic\": \"machine learning\"})\n",
    "print(f\"Type: {type(result_without_parser)}\")\n",
    "print(f\"Content: {result_without_parser.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\n--- With parser ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'str'>\n",
      "Content: \n"
     ]
    }
   ],
   "source": [
    "result_with_parser = parsing_chain.invoke({\"topic\": \"machine learning\"})\n",
    "print(f\"Type: {type(result_with_parser)}\")\n",
    "print(f\"Content: {result_with_parser}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CHAT PROMPT CHAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüí¨ 3. Chat Prompt Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding example: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful coding instructor. Provide clear, practical examples.\"),\n",
    "    (\"human\", \"Show me how to {task} in Python with a simple example.\")\n",
    "])\n",
    "\n",
    "# Create a chat chain\n",
    "chat_chain = chat_prompt | llm | string_parser\n",
    "\n",
    "# Use the chat chain\n",
    "coding_result = chat_chain.invoke({\"task\": \"read a JSON file\"})\n",
    "print(f\"Coding example: {coding_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MULTI-STEP CHAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüîÑ 4. Multi-step Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated subtopic: Explainable AI\n",
      "Explanation: Explainable AI (XAI) refers to methods and techniques that make the decisions and predictions of AI models understandable to humans. Its primary goal is to shed light on *why* an AI arrived at a particular conclusion, rather than just *what* the conclusion is. This transparency helps build trust, debug issues, and ensure ethical and fair AI deployment.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Generate a topic\n",
    "topic_prompt = PromptTemplate(\n",
    "    input_variables=[\"subject\"],\n",
    "    template=\"Suggest an interesting subtopic related to {subject}. Respond with just the subtopic name.\"\n",
    ")\n",
    "\n",
    "# Step 2: Explain the topic\n",
    "explain_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in 2-3 sentences.\"\n",
    ")\n",
    "\n",
    "# Create individual chains\n",
    "topic_chain = topic_prompt | llm | string_parser\n",
    "explain_chain = explain_prompt | llm | string_parser\n",
    "\n",
    "# Combine them manually\n",
    "\n",
    "\n",
    "def multi_step_process(subject):\n",
    "    # Step 1: Get a subtopic\n",
    "    subtopic = topic_chain.invoke({\"subject\": subject})\n",
    "    print(f\"Generated subtopic: {subtopic}\")\n",
    "\n",
    "    # Step 2: Explain the subtopic\n",
    "    explanation = explain_chain.invoke({\"topic\": subtopic})\n",
    "    return explanation\n",
    "\n",
    "\n",
    "# Use the multi-step process\n",
    "result = multi_step_process(\"artificial intelligence\")\n",
    "print(f\"Explanation: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CHAIN WITH CUSTOM FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\n‚öôÔ∏è 5. Chain with Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom chain result: üìö EXPLANATION: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def format_input(input_dict):\n",
    "    \"\"\"Custom function to preprocess input\"\"\"\n",
    "    topic = input_dict[\"topic\"]\n",
    "    formatted_topic = topic.upper().replace(\" \", \"_\")\n",
    "    return {\"formatted_topic\": formatted_topic, \"original\": topic}\n",
    "\n",
    "\n",
    "def format_output(text):\n",
    "    \"\"\"Custom function to postprocess output\"\"\"\n",
    "    return f\"üìö EXPLANATION: {text.strip()}\"\n",
    "\n",
    "\n",
    "# Create custom runnables\n",
    "input_formatter = RunnableLambda(format_input)\n",
    "output_formatter = RunnableLambda(format_output)\n",
    "\n",
    "# Create a prompt that uses the formatted input\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"formatted_topic\", \"original\"],\n",
    "    template=\"Topic code: {formatted_topic}\\nProvide a brief explanation of {original}:\"\n",
    ")\n",
    "\n",
    "# Create the full chain with custom functions\n",
    "custom_chain = input_formatter | custom_prompt | llm | string_parser | output_formatter\n",
    "\n",
    "# Use the custom chain\n",
    "custom_result = custom_chain.invoke({\"topic\": \"neural networks\"})\n",
    "print(f\"Custom chain result: {custom_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CONDITIONAL CHAINS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüéØ 6. Conditional Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BEGINNER LEVEL ---\n",
      "Result: \n",
      "\n",
      "--- INTERMEDIATE LEVEL ---\n",
      "Result: \n",
      "\n",
      "--- ADVANCED LEVEL ---\n",
      "Result: \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def route_by_difficulty(input_dict):\n",
    "    \"\"\"Route to different prompts based on difficulty level\"\"\"\n",
    "    difficulty = input_dict.get(\"difficulty\", \"beginner\")\n",
    "    topic = input_dict[\"topic\"]\n",
    "\n",
    "    if difficulty == \"beginner\":\n",
    "        template = \"Explain {topic} in very simple terms, like you're talking to a child.\"\n",
    "    elif difficulty == \"intermediate\":\n",
    "        template = \"Explain {topic} with some technical details, but keep it accessible.\"\n",
    "    else:  # advanced\n",
    "        template = \"Provide a detailed technical explanation of {topic} with advanced concepts.\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"topic\"],\n",
    "        template=template\n",
    "    )\n",
    "\n",
    "    return prompt.format(topic=topic)\n",
    "\n",
    "\n",
    "# Create conditional chain\n",
    "conditional_chain = RunnableLambda(route_by_difficulty) | llm | string_parser\n",
    "\n",
    "# Test with different difficulty levels\n",
    "for level in [\"beginner\", \"intermediate\", \"advanced\"]:\n",
    "    print(f\"\\n--- {level.upper()} LEVEL ---\")\n",
    "    result = conditional_chain.invoke({\n",
    "        \"topic\": \"quantum computing\",\n",
    "        \"difficulty\": level\n",
    "    })\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. KEY INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüìã Key Information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ | operator: Chains components together (pipe operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ LCEL: LangChain Expression Language for building chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ RunnableLambda: Wrap custom functions in chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ StrOutputParser: Extracts string content from LLM responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Chains process data sequentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Custom functions enable preprocessing and postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Conditional logic allows dynamic chain behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüéØ Next step: Learn intermediate LCEL features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
