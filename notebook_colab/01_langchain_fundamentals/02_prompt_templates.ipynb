{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-google-genai python-dotenv langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prompt Templates with LangChain and Google Gemini\n",
    "================================================\n",
    "\n",
    "In this notebook, we learn:\n",
    "- Creating reusable prompt templates\n",
    "- Using variables in prompts\n",
    "- Difference between PromptTemplate and ChatPromptTemplate\n",
    "- Formatting and using templates effectively\n",
    "\n",
    "Official documentation:\n",
    "- LangChain Prompt Templates: https://python.langchain.com/docs/concepts/prompt_templates/\n",
    "- LangChain Templates Guide: https://python.langchain.com/docs/how_to/prompts_composition/\n",
    "\"\"\"\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotPromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Gemini 2.5 Flash model initialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BASIC PROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüìù 1. Basic Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt: Explain machine learning in simple terms for a beginner.\n",
      "Response: Imagine you want to teach a computer something, but instead of giving it a super detailed, step-by-step instruction manual for *every single possibility*, you let it figure things out for itself by showing it examples. That's essentially what Machine Learning (ML) is!\n",
      "\n",
      "Let's break it down with a simple analogy:\n",
      "\n",
      "**Think about how a child learns what a \"dog\" is:**\n",
      "\n",
      "1.  **You show the child many pictures of dogs.** Some are big, some are small, different breeds, different colors. You say, \"This is a dog.\" (This is your **Data**).\n",
      "2.  **You also show them pictures of other animals** ‚Äì cats, birds, cows ‚Äì and say, \"This is *not* a dog.\" (More **Data**, helping them understand what *isn't* a dog).\n",
      "3.  **The child's brain starts to find patterns.** It learns that dogs usually have fur, four legs, a tail, certain types of faces, and they bark. (This is the **Learning Algorithm** at work).\n",
      "4.  **After seeing many examples, the child builds an internal \"model\" of what a dog is.**\n",
      "5.  **Later, you show the child a brand new animal they've never seen before.** Based on their learned patterns, they can say, \"That's a dog!\" or \"That's a cat!\" (This is the **Prediction** or **Decision**).\n",
      "\n",
      "**Machine Learning works the same way for computers:**\n",
      "\n",
      "*   **Instead of a child's brain, we use a computer program (the \"algorithm\").**\n",
      "*   **Instead of pictures, we feed it lots of digital \"data\"** ‚Äì could be images, text, numbers, sounds, etc.\n",
      "*   **The computer \"trains\" itself by looking for patterns in this data.** It's not explicitly programmed for every single rule (\"if it has fur AND barks AND has pointy ears, it's a dog\"). Instead, it figures out these rules by itself from the examples.\n",
      "*   **Once trained, the computer can make predictions or decisions** on new, unseen data.\n",
      "\n",
      "**Key Ingredients of Machine Learning:**\n",
      "\n",
      "1.  **Data:** This is the \"experience\" the computer learns from. The more good data, the better it learns.\n",
      "2.  **Algorithm:** This is the \"learning method\" or the \"brain\" of the computer program that processes the data and finds patterns.\n",
      "3.  **Training:** This is the process of feeding the data to the algorithm so it can learn and build its \"model.\"\n",
      "\n",
      "**What can Machine Learning do?**\n",
      "\n",
      "*   **Recommendations:** \"People who bought this also liked...\" (Netflix, Amazon, Spotify)\n",
      "*   **Spam Filtering:** Identifying unwanted emails.\n",
      "*   **Image Recognition:** Tagging faces in photos, identifying objects in self-driving cars.\n",
      "*   **Voice Assistants:** Understanding your commands (Siri, Alexa, Google Assistant).\n",
      "*   **Medical Diagnosis:** Helping doctors identify diseases from scans.\n",
      "*   **Fraud Detection:** Spotting unusual credit card transactions.\n",
      "\n",
      "**In simple terms:**\n",
      "\n",
      "Machine Learning is about **teaching computers to learn from experience (data), just like we do, so they can make smart decisions and predictions without being told exactly what to do every step of the way.** It's giving computers the ability to adapt and improve over time.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a simple template with one variable\n",
    "basic_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms for a beginner.\"\n",
    ")\n",
    "\n",
    "# Format the template\n",
    "formatted_prompt = basic_template.format(topic=\"machine learning\")\n",
    "print(f\"Formatted prompt: {formatted_prompt}\")\n",
    "\n",
    "# Use with LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(f\"Response: {response.content}\")\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MULTI-VARIABLE PROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüìù 2. Multi-variable Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted prompt: Write a short explanation of artificial intelligence for high school students.\n",
      "Make sure to use appropriate language and examples.\n",
      "Response: ## What is Artificial Intelligence (AI)?\n",
      "\n",
      "Imagine you're trying to teach a really smart robot how to play a game, like chess or even just recognize a cat in a picture. Instead of telling it every single move or every single detail of what makes a cat a cat, you show it lots and lots of examples. Over time, the robot starts to figure out the patterns and rules on its own.\n",
      "\n",
      "That, in a nutshell, is Artificial Intelligence (AI)!\n",
      "\n",
      "**AI is when computers are programmed to \"think\" and \"learn\" in ways that mimic human intelligence.** It's about creating machines that can:\n",
      "\n",
      "1.  **Understand and process information:** Like understanding your voice commands or recognizing objects in a photo.\n",
      "2.  **Learn from experience:** Just like you get better at a game the more you play, AI systems improve by analyzing tons of data.\n",
      "3.  **Solve problems and make decisions:** From suggesting what movie you might like next to helping doctors diagnose diseases.\n",
      "\n",
      "### How Does it Work (Simply)?\n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "*   **Data is the Food:** AI systems are fed massive amounts of information ‚Äì called \"data.\" This could be millions of images, recordings of human speech, text from books, or records of purchases.\n",
      "*   **Patterns are the Clues:** The AI then uses complex algorithms (basically, really smart step-by-step instructions) to find patterns and relationships within that data.\n",
      "*   **Learning is the Result:** Once it finds these patterns, it can use them to make predictions, classify new information, or perform tasks without being explicitly programmed for every single scenario. For example, if it's seen a million pictures of cats, it can then correctly identify a new picture of a cat it's never seen before.\n",
      "\n",
      "### Where Do You See AI in Your Daily Life?\n",
      "\n",
      "AI isn't some futuristic sci-fi concept; it's all around you right now!\n",
      "\n",
      "*   **Your Smartphone:**\n",
      "    *   **Voice Assistants:** Siri, Google Assistant, Alexa understand what you say and respond.\n",
      "    *   **Predictive Text:** When your phone suggests the next word you're typing.\n",
      "    *   **Face Recognition:** Unlocking your phone or tagging friends in photos.\n",
      "*   **Streaming Services:**\n",
      "    *   **Recommendations:** Netflix, Spotify, and YouTube suggest movies, music, or videos you might like based on what you've watched or listened to before.\n",
      "*   **Online Shopping:**\n",
      "    *   **Product Suggestions:** Websites like Amazon suggest items based on your browsing and purchase history.\n",
      "*   **Video Games:**\n",
      "    *   **Non-Player Characters (NPCs):** The computer-controlled characters in games that seem to react intelligently to your actions.\n",
      "*   **Social Media:**\n",
      "    *   **Content Filtering:** Helping to identify spam or inappropriate content.\n",
      "\n",
      "### Why is AI a Big Deal?\n",
      "\n",
      "AI isn't just about convenience; it's a powerful tool that helps us solve incredibly complex problems:\n",
      "\n",
      "*   **Medicine:** Helping doctors diagnose diseases earlier and more accurately.\n",
      "*   **Science:** Analyzing vast amounts of data to understand climate change or discover new materials.\n",
      "*   **Safety:** Powering self-driving cars and helping systems detect fraud.\n",
      "\n",
      "In essence, AI is about creating smart tools that can learn and adapt, making our lives easier, safer, and opening up new possibilities for the future. It's not magic, but it's pretty close to giving computers a form of \"brainpower.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create template with multiple variables\n",
    "multi_template = PromptTemplate(\n",
    "    input_variables=[\"subject\", \"audience\", \"length\"],\n",
    "    template=\"\"\"Write a {length} explanation of {subject} for {audience}.\n",
    "Make sure to use appropriate language and examples.\"\"\"\n",
    ")\n",
    "\n",
    "# Format with multiple variables\n",
    "formatted_multi = multi_template.format(\n",
    "    subject=\"artificial intelligence\",\n",
    "    audience=\"high school students\",\n",
    "    length=\"short\"\n",
    ")\n",
    "\n",
    "print(f\"Formatted prompt: {formatted_multi}\")\n",
    "\n",
    "response = llm.invoke(formatted_multi)\n",
    "print(f\"Response: {response.content}\")\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CHAT PROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüí¨ 3. Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted messages: [SystemMessage(content='You are a helpful coding assistant. Always provide clear, well-commented code examples.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Show me how to read a CSV file in Python.', additional_kwargs={}, response_metadata={})]\n",
      "Response: Reading CSV (Comma Separated Values) files in Python is a very common task. Python provides excellent built-in capabilities for this, and there's also a powerful external library, `pandas`, which is often preferred for data analysis.\n",
      "\n",
      "Here, I'll show you how to read CSV files using both approaches:\n",
      "\n",
      "1.  **Using Python's built-in `csv` module:** This is suitable for basic reading and writing, especially when you don't need extensive data manipulation.\n",
      "2.  **Using the `pandas` library:** This is the industry standard for data analysis and manipulation in Python. It's highly optimized and provides powerful tools for working with tabular data.\n",
      "\n",
      "---\n",
      "\n",
      "### First, let's create a sample CSV file\n",
      "\n",
      "Before running the code examples, create a file named `employees.csv` in the same directory as your Python script, and paste the following content into it:\n",
      "\n",
      "```csv\n",
      "ID,Name,Department,Salary,HireDate\n",
      "101,Alice Smith,HR,60000,2020-01-15\n",
      "102,Bob Johnson,IT,75000,2019-03-22\n",
      "103,Charlie Brown,Finance,80000,2021-07-01\n",
      "104,Diana Miller,HR,62000,2022-09-10\n",
      "105,Eve Davis,IT,78000,2020-11-05\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Method 1: Using Python's built-in `csv` module\n",
      "\n",
      "The `csv` module allows you to read and write CSV files without needing any external libraries. It handles various CSV formats and quoting rules automatically.\n",
      "\n",
      "#### 1.1 Reading rows as lists (`csv.reader`)\n",
      "\n",
      "This is the most basic way. Each row is read as a list of strings.\n",
      "\n",
      "```python\n",
      "import csv\n",
      "\n",
      "def read_csv_as_lists(filepath):\n",
      "    \"\"\"\n",
      "    Reads a CSV file and prints each row as a list.\n",
      "    The first row is treated as headers.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Open the CSV file in read mode ('r')\n",
      "        # 'newline=''' is important to prevent issues with line endings\n",
      "        with open(filepath, 'r', newline='', encoding='utf-8') as csvfile:\n",
      "            # Create a csv.reader object\n",
      "            csv_reader = csv.reader(csvfile)\n",
      "\n",
      "            # Read the header row\n",
      "            headers = next(csv_reader)\n",
      "            print(f\"Headers: {headers}\")\n",
      "            print(\"-\" * 30)\n",
      "\n",
      "            # Iterate over each row in the CSV file\n",
      "            for row in csv_reader:\n",
      "                # Each 'row' is a list of strings\n",
      "                print(f\"Row: {row}\")\n",
      "\n",
      "                # You can access specific columns by index\n",
      "                # For example, to get the name and salary:\n",
      "                # name = row[headers.index('Name')]\n",
      "                # salary = row[headers.index('Salary')]\n",
      "                # print(f\"  Name: {name}, Salary: {salary}\")\n",
      "\n",
      "    except FileNotFoundError:\n",
      "        print(f\"Error: The file '{filepath}' was not found.\")\n",
      "    except Exception as e:\n",
      "        print(f\"An error occurred: {e}\")\n",
      "\n",
      "# --- Example Usage ---\n",
      "print(\"--- Reading CSV using csv.reader (rows as lists) ---\")\n",
      "read_csv_as_lists('employees.csv')\n",
      "print(\"\\n\")\n",
      "```\n",
      "\n",
      "#### 1.2 Reading rows as dictionaries (`csv.DictReader`)\n",
      "\n",
      "This is often more convenient as it treats each row as a dictionary where keys are the column headers. This makes accessing data by column name much easier and more readable.\n",
      "\n",
      "```python\n",
      "import csv\n",
      "\n",
      "def read_csv_as_dictionaries(filepath):\n",
      "    \"\"\"\n",
      "    Reads a CSV file and prints each row as a dictionary.\n",
      "    Keys are derived from the header row.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with open(filepath, 'r', newline='', encoding='utf-8') as csvfile:\n",
      "            # Create a csv.DictReader object\n",
      "            # It automatically uses the first row as fieldnames (keys)\n",
      "            csv_dict_reader = csv.DictReader(csvfile)\n",
      "\n",
      "            # Print the fieldnames (headers)\n",
      "            print(f\"Fieldnames (Headers): {csv_dict_reader.fieldnames}\")\n",
      "            print(\"-\" * 30)\n",
      "\n",
      "            # Iterate over each row in the CSV file\n",
      "            for row in csv_dict_reader:\n",
      "                # Each 'row' is now a dictionary\n",
      "                print(f\"Row: {row}\")\n",
      "\n",
      "                # You can access specific columns by their header name\n",
      "                print(f\"  Name: {row['Name']}, Department: {row['Department']}\")\n",
      "                print(f\"  Salary: {row['Salary']}\")\n",
      "\n",
      "    except FileNotFoundError:\n",
      "        print(f\"Error: The file '{filepath}' was not found.\")\n",
      "    except Exception as e:\n",
      "        print(f\"An error occurred: {e}\")\n",
      "\n",
      "# --- Example Usage ---\n",
      "print(\"--- Reading CSV using csv.DictReader (rows as dictionaries) ---\")\n",
      "read_csv_as_dictionaries('employees.csv')\n",
      "print(\"\\n\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Method 2: Using the `pandas` library\n",
      "\n",
      "`pandas` is a powerful and widely used library for data manipulation and analysis. It excels at reading and working with tabular data, including CSV files. If you're doing any kind of data analysis, `pandas` is usually the way to go.\n",
      "\n",
      "**Installation:**\n",
      "If you don't have `pandas` installed, you can install it via pip:\n",
      "`pip install pandas`\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "def read_csv_with_pandas(filepath):\n",
      "    \"\"\"\n",
      "    Reads a CSV file into a pandas DataFrame and performs some basic operations.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Read the CSV file into a DataFrame\n",
      "        # pd.read_csv() is very powerful and has many options\n",
      "        df = pd.read_csv(filepath)\n",
      "\n",
      "        print(\"--- Reading CSV using pandas ---\")\n",
      "        print(\"DataFrame created successfully.\")\n",
      "\n",
      "        # Display the first few rows of the DataFrame\n",
      "        print(\"\\nHead of the DataFrame:\")\n",
      "        print(df.head())\n",
      "\n",
      "        # Display basic information about the DataFrame\n",
      "        print(\"\\nDataFrame Info:\")\n",
      "        df.info()\n",
      "\n",
      "        # Display descriptive statistics for numerical columns\n",
      "        print(\"\\nDescriptive Statistics:\")\n",
      "        print(df.describe())\n",
      "\n",
      "        # Access a specific column\n",
      "        print(\"\\nNames of employees:\")\n",
      "        print(df['Name'])\n",
      "\n",
      "        # Filter data (e.g., employees in the 'IT' department)\n",
      "        print(\"\\nEmployees in IT Department:\")\n",
      "        it_employees = df[df['Department'] == 'IT']\n",
      "        print(it_employees)\n",
      "\n",
      "        # Calculate average salary\n",
      "        # Note: 'Salary' might be read as a string, convert to numeric if needed\n",
      "        df['Salary'] = pd.to_numeric(df['Salary'])\n",
      "        average_salary = df['Salary'].mean()\n",
      "        print(f\"\\nAverage Salary: ${average_salary:,.2f}\")\n",
      "\n",
      "    except FileNotFoundError:\n",
      "        print(f\"Error: The file '{filepath}' was not found.\")\n",
      "    except ImportError:\n",
      "        print(\"Error: pandas library not found. Please install it using 'pip install pandas'.\")\n",
      "    except Exception as e:\n",
      "        print(f\"An error occurred: {e}\")\n",
      "\n",
      "# --- Example Usage ---\n",
      "read_csv_with_pandas('employees.csv')\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Choosing the Right Method\n",
      "\n",
      "*   **`csv` module (built-in):**\n",
      "    *   **Pros:** No external dependencies, good for simple reading/writing, handles various CSV dialects.\n",
      "    *   **Cons:** More verbose for complex tasks, requires manual type conversion (all data is read as strings), not optimized for large-scale data manipulation.\n",
      "    *   **Best for:** Small scripts, processing CSVs where you only need to iterate once, or when you explicitly want to avoid external libraries.\n",
      "\n",
      "*   **`pandas` library:**\n",
      "    *   **Pros:** Extremely powerful for data analysis, handles large datasets efficiently, automatic type inference, rich API for data cleaning, transformation, and analysis.\n",
      "    *   **Cons:** Requires installation, can be overkill for very simple tasks.\n",
      "    *   **Best for:** Any task involving data analysis, larger CSV files, integration with other data sources, machine learning preprocessing.\n",
      "\n",
      "For most real-world data tasks in Python, `pandas` is the preferred and recommended tool.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful coding assistant. Always provide clear, well-commented code examples.\"),\n",
    "    (\"human\", \"Show me how to {task} in Python.\")\n",
    "])\n",
    "\n",
    "# Format the chat template\n",
    "formatted_chat = chat_template.format_messages(task=\"read a CSV file\")\n",
    "print(f\"Formatted messages: {formatted_chat}\")\n",
    "\n",
    "# Use with LLM\n",
    "response = llm.invoke(formatted_chat)\n",
    "print(f\"Response: {response.content}\")\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FEW-SHOT PROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüéØ 4. Few-shot Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted few-shot prompt:\n",
      "You are a helpful assistant. Answer questions clearly and concisely.\n",
      "\n",
      "Input: What is 2+2?\n",
      "Output: 2+2 equals 4\n",
      "\n",
      "Input: What is the capital of France?\n",
      "Output: The capital of France is Paris\n",
      "\n",
      "Input: What color is the sky?\n",
      "Output: The sky is typically blue during clear weather\n",
      "\n",
      "Input: What is the capital of Japan?\n",
      "Output:\n",
      "\n",
      "--------------------------------------------------\n",
      "Response: The capital of Japan is Tokyo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define examples for few-shot learning\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"What is 2+2?\",\n",
    "        \"output\": \"2+2 equals 4\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": \"The capital of France is Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What color is the sky?\",\n",
    "        \"output\": \"The sky is typically blue during clear weather\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create example template\n",
    "example_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "# Create few-shot prompt template\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_template,\n",
    "    prefix=\"You are a helpful assistant. Answer questions clearly and concisely.\",\n",
    "    suffix=\"Input: {input}\\nOutput:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# Test the few-shot template\n",
    "test_input = \"What is the capital of Japan?\"\n",
    "formatted_prompt = few_shot_prompt.format(input=test_input)\n",
    "print(f\"Formatted few-shot prompt:\\n{formatted_prompt}\")\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(f\"Response: {response.content}\")\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CONDITIONAL TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüîÑ 5. Conditional Template Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: Imagine you want to teach a computer to recognize a cat. How would you do it? You could list rules: \"It has pointy ears, whiskers, fur, a tail.\" But what about a cat curled up? Or seen from the side? Or a hairless cat? This rule-based approach quickly becomes complex and brittle.\n",
      "\n",
      "This is where **Neural Networks** come in. They are a powerful type of machine learning algorithm inspired by the way the human brain works. Instead of being explicitly programmed with rules, they *learn* from examples.\n",
      "\n",
      "---\n",
      "\n",
      "## What is a Neural Network?\n",
      "\n",
      "At its core, a neural network is a system of interconnected \"neurons\" (or \"nodes\") organized in layers. It's designed to recognize patterns and make predictions by learning from data.\n",
      "\n",
      "Let's break down the key components:\n",
      "\n",
      "1.  **The Neuron (Node): The Basic Building Block**\n",
      "    *   Imagine a tiny decision-making unit.\n",
      "    *   It receives several **inputs** (numbers).\n",
      "    *   Each input is multiplied by a **weight** (another number). Weights determine the importance of an input.\n",
      "    *   All these weighted inputs are summed up, and a **bias** (yet another number) is added.\n",
      "    *   This sum then passes through an **activation function**. This function decides whether the neuron \"fires\" (sends a signal) or not, and what value it sends. It introduces non-linearity, allowing the network to learn complex patterns.\n",
      "    *   Finally, the neuron produces an **output**.\n",
      "\n",
      "    *Analogy:* Think of a single neuron as a light switch that only turns on if enough \"electrical current\" (sum of weighted inputs + bias) passes through it.\n",
      "\n",
      "2.  **Layers of Neurons:**\n",
      "    Neural networks are typically organized into three types of layers:\n",
      "\n",
      "    *   **Input Layer:** This is where your raw data (e.g., pixels of an image, words in a sentence, numerical features) enters the network. Each neuron in this layer represents a feature of your input.\n",
      "    *   **Hidden Layers:** These are the \"brain\" of the network, where the magic happens. Neurons in hidden layers process the information from the previous layer, learn complex patterns, and extract features. A network can have one or many hidden layers (this is called a \"deep\" neural network).\n",
      "    *   **Output Layer:** This layer produces the final result of the network's processing. The number of neurons here depends on the task (e.g., one neuron for a \"yes/no\" answer, multiple neurons for classifying into several categories).\n",
      "\n",
      "3.  **Connections (Weights & Biases):**\n",
      "    *   Every neuron in one layer is connected to every neuron in the next layer.\n",
      "    *   Each connection has an associated **weight**. These weights are the parameters the network learns during training. A higher weight means that input has a stronger influence on the next neuron.\n",
      "    *   Each neuron also has a **bias**, which is like an additional input that always has a value of 1, but with its own learnable weight. It allows the activation function to be shifted, giving the neuron more flexibility.\n",
      "\n",
      "---\n",
      "\n",
      "## How Does a Neural Network \"Learn\"?\n",
      "\n",
      "Learning in a neural network is like a child learning to identify objects through trial and error, with corrections from an adult.\n",
      "\n",
      "1.  **Training Data:** You feed the network a large dataset of examples, where each example has an input (e.g., an image of a cat) and a known correct output (e.g., the label \"cat\").\n",
      "\n",
      "2.  **Forward Pass:**\n",
      "    *   The input data enters the input layer.\n",
      "    *   It travels through the hidden layers, with each neuron performing its weighted sum and activation function.\n",
      "    *   Finally, the output layer produces a prediction.\n",
      "\n",
      "3.  **Error Calculation (Loss Function):**\n",
      "    *   The network's prediction is compared to the actual correct output.\n",
      "    *   A \"loss function\" calculates how wrong the prediction was. A higher loss means a worse prediction.\n",
      "\n",
      "4.  **Backward Pass (Backpropagation):**\n",
      "    *   This is the core of learning. The error calculated in the previous step is propagated backward through the network, from the output layer to the input layer.\n",
      "    *   Based on this error, the network intelligently adjusts the **weights** and **biases** of each connection and neuron. The goal is to make these adjustments in such a way that the network's next prediction will be closer to the correct answer, thus reducing the loss.\n",
      "\n",
      "5.  **Iteration:**\n",
      "    *   This entire process (forward pass, error calculation, backward pass, weight adjustment) is repeated thousands or millions of times with different examples from the training data.\n",
      "    *   Over time, the network gradually refines its weights and biases, becoming increasingly accurate at making predictions for new, unseen data.\n",
      "\n",
      "---\n",
      "\n",
      "## Why Are Neural Networks Powerful?\n",
      "\n",
      "*   **Pattern Recognition:** Excellent at finding complex, non-linear patterns in data that humans or traditional algorithms might miss.\n",
      "*   **Adaptability:** They can learn and adapt to new data without being explicitly reprogrammed.\n",
      "*   **Feature Extraction:** Deep neural networks can automatically learn relevant features from raw data, rather than requiring humans to hand-engineer them.\n",
      "*   **Scalability:** With enough data and computational power, they can handle incredibly complex tasks.\n",
      "\n",
      "---\n",
      "\n",
      "## Real-World Examples of Neural Networks:\n",
      "\n",
      "1.  **Image Recognition (e.g., Self-Driving Cars, Facial Recognition):**\n",
      "    *   **How it works:** A neural network (often a Convolutional Neural Network or CNN) is trained on millions of images labeled with objects (cars, pedestrians, traffic signs, faces).\n",
      "    *   **Example:** When a self-driving car's camera sees an image, the network processes the pixels, identifies objects, their positions, and predicts their movement. Similarly, your phone's facial recognition system identifies your face from a new image by comparing it to learned patterns.\n",
      "\n",
      "2.  **Natural Language Processing (e.g., Chatbots, Language Translation, Spam Filters):**\n",
      "    *   **How it works:** Recurrent Neural Networks (RNNs) or Transformer networks are trained on vast amounts of text data. They learn the rules of language, context, and meaning.\n",
      "    *   **Example:** When you type a query into a chatbot, the network understands the intent of your words and generates a relevant response. Google Translate uses neural networks to translate sentences between languages, understanding context rather than just word-for-word translation. Your email spam filter identifies suspicious patterns in emails to flag them as spam.\n",
      "\n",
      "3.  **Recommendation Systems (e.g., Netflix, Amazon, Spotify):**\n",
      "    *   **How it works:** Neural networks learn your preferences, viewing history, and interaction patterns, as well as those of other users.\n",
      "    *   **Example:** When you watch movies on Netflix, the network analyzes what you've watched, rated, and even how long you hovered over certain titles. It then compares your preferences to others and suggests new movies or shows you're likely to enjoy. Amazon's \"Customers who bought this also bought...\" works similarly.\n",
      "\n",
      "4.  **Voice Assistants (e.g., Siri, Alexa, Google Assistant):**\n",
      "    *   **How it works:** Neural networks are crucial for both \"speech-to-text\" (converting audio to written words) and \"text-to-speech\" (generating spoken responses).\n",
      "    *   **Example:** When you say, \"Hey Siri, what's the weather like?\", a neural network first takes your audio input, recognizes your speech patterns, and converts it into text. Another network then processes that text to understand your command and fetch the weather information.\n",
      "\n",
      "5.  **Fraud Detection (e.g., Credit Card Transactions):**\n",
      "    *   **How it works:** Networks are trained on historical transaction data, including both legitimate and fraudulent activities. They learn to identify subtle, complex patterns that distinguish fraudulent transactions.\n",
      "    *   **Example:** When you make a credit card purchase, a neural network quickly analyzes factors like the transaction amount, location, time, merchant, and your past spending habits. If it detects unusual patterns (e.g., a large purchase far from your usual location), it might flag the transaction as potentially fraudulent.\n",
      "\n",
      "In essence, neural networks are powerful tools that allow computers to learn from data, recognize complex patterns, and make intelligent decisions or predictions, mimicking some of the cognitive abilities of the human brain.\n",
      "Summary: Artificial intelligence is a rapidly expanding field dedicated to developing machines that can perform tasks traditionally requiring human intellect. These capabilities encompass learning, reasoning, problem-solving, and comprehending natural language.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create templates for different use cases\n",
    "templates = {\n",
    "    \"explain\": PromptTemplate(\n",
    "        input_variables=[\"concept\"],\n",
    "        template=\"Provide a clear explanation of {concept} with real-world examples.\"\n",
    "    ),\n",
    "    \"summarize\": PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"Summarize the following text in 2-3 sentences: {text}\"\n",
    "    ),\n",
    "    \"translate\": PromptTemplate(\n",
    "        input_variables=[\"text\", \"language\"],\n",
    "        template=\"Translate the following text to {language}: {text}\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Function to use appropriate template\n",
    "\n",
    "\n",
    "def process_request(request_type, **kwargs):\n",
    "    if request_type in templates:\n",
    "        template = templates[request_type]\n",
    "        formatted_prompt = template.format(**kwargs)\n",
    "        return llm.invoke(formatted_prompt)\n",
    "    else:\n",
    "        return \"Unknown request type\"\n",
    "\n",
    "\n",
    "# Test different templates\n",
    "explain_response = process_request(\"explain\", concept=\"neural networks\")\n",
    "print(f\"Explanation: {explain_response.content}\")\n",
    "\n",
    "summarize_response = process_request(\n",
    "    \"summarize\",\n",
    "    text=\"Artificial intelligence is a rapidly growing field that focuses on creating machines capable of performing tasks that typically require human intelligence. This includes learning, reasoning, problem-solving, and understanding natural language.\"\n",
    ")\n",
    "print(f\"Summary: {summarize_response.content}\")\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TEMPLATE COMPOSITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\nüîß 6. Template Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composed prompt: You are an expert in data science.\n",
      "\n",
      "Your task is to analyze a dataset. Here are the details: The dataset contains customer purchase data with columns: date, product, price, customer_id\n",
      "Response: Excellent! As an expert in data science, I'd approach this customer purchase dataset with a structured methodology, moving from data understanding to deep insights and actionable recommendations.\n",
      "\n",
      "Here's my comprehensive analysis plan for a dataset with `date`, `product`, `price`, and `customer_id` columns:\n",
      "\n",
      "---\n",
      "\n",
      "## Data Analysis Plan: Customer Purchase Data\n",
      "\n",
      "**Dataset:** Customer purchase records\n",
      "**Columns:** `date`, `product`, `price`, `customer_id`\n",
      "\n",
      "### Phase 1: Data Understanding & Preprocessing (EDA Foundation)\n",
      "\n",
      "The first step is always to get a thorough understanding of the data's quality, structure, and basic characteristics.\n",
      "\n",
      "1.  **Initial Data Inspection:**\n",
      "    *   **Load Data:** Use Pandas (`pd.read_csv` or similar).\n",
      "    *   **`df.head()` / `df.sample()`:** Glimpse the first few rows and a random sample to understand the data format.\n",
      "    *   **`df.info()`:** Check data types (`date` should be datetime, `price` numeric, `customer_id` and `product` typically object/string). Identify non-null counts.\n",
      "    *   **`df.describe()`:** Get summary statistics for numerical columns (`price`). This will give us min, max, mean, std, quartiles.\n",
      "    *   **`df.shape`:** Number of rows (transactions) and columns.\n",
      "\n",
      "2.  **Data Cleaning & Type Conversion:**\n",
      "    *   **Date Column:** Convert `date` to datetime objects (`pd.to_datetime`). This is crucial for any time-series analysis.\n",
      "    *   **Price Column:** Ensure `price` is a numeric type (float or int). Handle any non-numeric entries if present (e.g., 'N/A', '$').\n",
      "    *   **Missing Values:**\n",
      "        *   `df.isnull().sum()`: Identify columns with missing values.\n",
      "        *   **Strategy:** For purchase data, missing `price`, `product`, or `customer_id` usually means an incomplete or invalid transaction. Depending on the volume, these rows might be dropped or investigated. Missing `date` would also be critical.\n",
      "    *   **Duplicates:**\n",
      "        *   `df.duplicated().sum()`: Check for identical rows. A duplicate transaction might indicate data entry errors or a true repeated purchase. Investigate if the count is high.\n",
      "    *   **Outliers:**\n",
      "        *   **`price`:** Visualize distribution (histogram, box plot). Identify unusually high or low prices. Decide if they are legitimate high-value purchases or data entry errors.\n",
      "\n",
      "### Phase 2: Exploratory Data Analysis (EDA) - Deeper Dive\n",
      "\n",
      "Now that the data is clean, we can start extracting meaningful patterns.\n",
      "\n",
      "1.  **Feature Engineering (Creating New Variables):**\n",
      "    *   **From `date`:**\n",
      "        *   `year`, `month`, `day_of_week`, `hour` (if `date` includes time).\n",
      "        *   `transaction_day`: A simplified date without time for daily aggregations.\n",
      "    *   **From `price`:** (No direct new features, but used in aggregations).\n",
      "    *   **From `customer_id`:** (No direct new features, but used in aggregations).\n",
      "    *   **From `product`:** (No direct new features, but used in aggregations).\n",
      "\n",
      "2.  **Univariate Analysis:**\n",
      "    *   **`price`:**\n",
      "        *   Distribution: Histogram, KDE plot to see the shape of prices.\n",
      "        *   Total Revenue: `df['price'].sum()`\n",
      "        *   Average Transaction Value: `df['price'].mean()`\n",
      "    *   **`product`:**\n",
      "        *   Number of unique products: `df['product'].nunique()`.\n",
      "        *   Top N most purchased products: `df['product'].value_counts().head(N)`.\n",
      "        *   Least N purchased products: `df['product'].value_counts().tail(N)`.\n",
      "    *   **`customer_id`:**\n",
      "        *   Number of unique customers: `df['customer_id'].nunique()`.\n",
      "        *   Distribution of transactions per customer: `df['customer_id'].value_counts()` (histogram).\n",
      "    *   **`date`:**\n",
      "        *   Transactions over time: Plot `df['date'].value_counts()` (daily, weekly, monthly). Identify peak purchase periods, trends, and seasonality.\n",
      "        *   Purchases by day of week, month, hour: Bar plots to show shopping patterns.\n",
      "\n",
      "3.  **Bivariate & Multivariate Analysis:**\n",
      "    *   **Revenue Trends:**\n",
      "        *   Total daily/weekly/monthly revenue over time. (Line plot: `date` vs. `sum(price)`).\n",
      "        *   Average transaction value over time.\n",
      "    *   **Product Performance:**\n",
      "        *   Revenue by product: `df.groupby('product')['price'].sum().sort_values(ascending=False).head(N)`.\n",
      "        *   Average price per product: `df.groupby('product')['price'].mean()`.\n",
      "        *   Product popularity over time.\n",
      "    *   **Customer Spending:**\n",
      "        *   Average spending per customer.\n",
      "        *   Total spending per customer.\n",
      "        *   Number of purchases per customer.\n",
      "    *   **Price vs. Product:** Are certain products consistently higher/lower priced?\n",
      "    *   **Time-based Product Analysis:** Which products sell best during specific months or days of the week?\n",
      "\n",
      "### Phase 3: Advanced Analysis & Modeling\n",
      "\n",
      "This phase moves beyond simple aggregations to derive deeper insights for strategic decision-making.\n",
      "\n",
      "1.  **Customer Segmentation (RFM Analysis):**\n",
      "    *   **Recency:** Days since the customer's last purchase.\n",
      "    *   **Frequency:** Total number of purchases made by the customer.\n",
      "    *   **Monetary:** Total amount of money spent by the customer.\n",
      "    *   **Process:**\n",
      "        *   Calculate R, F, M for each unique `customer_id`.\n",
      "        *   Segment customers into groups (e.g., \"Champions,\" \"Loyal Customers,\" \"At Risk,\" \"New Customers\") based on RFM scores (e.g., quintile scoring).\n",
      "    *   **Benefits:** Identify valuable customers, target specific segments with tailored marketing campaigns (e.g., retention for \"At Risk,\" loyalty programs for \"Champions\").\n",
      "    *   **Visualization:** Scatter plots of F vs. M, R vs. F, and R vs. M, colored by segment.\n",
      "\n",
      "2.  **Product Affinity / Market Basket Analysis:**\n",
      "    *   **Goal:** Discover which products are frequently purchased together.\n",
      "    *   **Method:** Use algorithms like Apriori or FP-Growth to find association rules (e.g., \"Customers who buy Product A also tend to buy Product B\").\n",
      "    *   **Benefits:** Cross-selling opportunities, product bundling, store layout optimization, recommendation systems.\n",
      "\n",
      "3.  **Customer Lifetime Value (CLV) Estimation:**\n",
      "    *   **Goal:** Predict the total revenue a business can expect from a customer throughout their relationship.\n",
      "    *   **Methods:** Simple heuristic (average purchase value * average purchase frequency * average customer lifespan) or more sophisticated probabilistic models (e.g., BG/NBD model for frequency, Gamma-Gamma model for monetary value).\n",
      "    *   **Benefits:** Prioritize customer acquisition and retention efforts, optimize marketing spend.\n",
      "\n",
      "4.  **Time Series Forecasting:**\n",
      "    *   **Goal:** Predict future sales/revenue.\n",
      "    *   **Method:** Aggregate sales data (e.g., daily, weekly, monthly total revenue) and apply time series models like ARIMA, Prophet, or Exponential Smoothing.\n",
      "    *   **Benefits:** Inventory management, resource planning, budgeting, identifying future trends.\n",
      "\n",
      "5.  **Cohort Analysis:**\n",
      "    *   **Goal:** Track the behavior of groups of customers (cohorts) over time.\n",
      "    *   **Method:** Group customers by their first purchase date (acquisition cohort) and then analyze their subsequent purchase behavior (retention, spending, product preferences) over time.\n",
      "    *   **Benefits:** Understand customer retention rates, identify if newer cohorts behave differently, assess the long-term impact of marketing campaigns.\n",
      "\n",
      "### Phase 4: Visualization & Reporting\n",
      "\n",
      "Effective communication of findings is paramount.\n",
      "\n",
      "1.  **Key Visualizations:**\n",
      "    *   Time series plots of sales/revenue (daily, weekly, monthly).\n",
      "    *   Bar charts for top products, sales by day of week/month.\n",
      "    *   Histograms for price distribution, transactions per customer.\n",
      "    *   Box plots for price distribution across different product categories (if product categories were available).\n",
      "    *   RFM segmentation charts.\n",
      "    *   Heatmaps for cohort analysis.\n",
      "    *   Network graphs for product associations.\n",
      "2.  **Dashboarding:** Create interactive dashboards (e.g., using Tableau, Power BI, Streamlit) to allow stakeholders to explore key metrics.\n",
      "3.  **Report Generation:**\n",
      "    *   **Executive Summary:** High-level overview of key findings and recommendations.\n",
      "    *   **Detailed Analysis:** Explanation of methodologies, visualizations, and insights for each phase.\n",
      "    *   **Actionable Recommendations:** Translate data insights into concrete business strategies.\n",
      "\n",
      "### Phase 5: Actionable Recommendations & Next Steps\n",
      "\n",
      "Based on the analysis, I would provide specific, data-driven recommendations:\n",
      "\n",
      "1.  **Targeted Marketing:**\n",
      "    *   Develop specific campaigns for different RFM segments (e.g., re-engagement campaigns for \"At Risk\" customers, loyalty rewards for \"Champions\").\n",
      "    *   Promote complementary products based on market basket analysis.\n",
      "2.  **Product Strategy:**\n",
      "    *   Identify underperforming products and consider promotions or discontinuation.\n",
      "    *   Capitalize on top-selling products by ensuring stock and visibility.\n",
      "    *   Bundle products that are frequently bought together.\n",
      "3.  **Pricing Strategy:**\n",
      "    *   Analyze price elasticity if additional data is available.\n",
      "    *   Optimize pricing for different product types or customer segments.\n",
      "4.  **Inventory Management:**\n",
      "    *   Use sales forecasts to optimize stock levels and reduce waste or stockouts.\n",
      "    *   Align inventory with seasonal trends identified.\n",
      "5.  **Customer Experience:**\n",
      "    *   Understand peak shopping times to optimize customer service staffing.\n",
      "    *   Personalize recommendations based on purchase history.\n",
      "6.  **Further Data Collection:**\n",
      "    *   Suggest adding customer demographics, product categories, marketing spend, or customer feedback to enrich future analyses.\n",
      "\n",
      "---\n",
      "\n",
      "This comprehensive plan ensures that we not only understand the current state of customer purchases but also gain predictive insights and actionable strategies to drive business growth and customer satisfaction.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create modular templates\n",
    "context_template = PromptTemplate(\n",
    "    input_variables=[\"domain\"],\n",
    "    template=\"You are an expert in {domain}.\"\n",
    ")\n",
    "\n",
    "task_template = PromptTemplate(\n",
    "    input_variables=[\"task\", \"details\"],\n",
    "    template=\"Your task is to {task}. Here are the details: {details}\"\n",
    ")\n",
    "\n",
    "# Combine templates\n",
    "\n",
    "\n",
    "def create_composed_prompt(domain, task, details):\n",
    "    context = context_template.format(domain=domain)\n",
    "    task_prompt = task_template.format(task=task, details=details)\n",
    "    return f\"{context}\\n\\n{task_prompt}\"\n",
    "\n",
    "\n",
    "# Use composed template\n",
    "composed_prompt = create_composed_prompt(\n",
    "    domain=\"data science\",\n",
    "    task=\"analyze a dataset\",\n",
    "    details=\"The dataset contains customer purchase data with columns: date, product, price, customer_id\"\n",
    ")\n",
    "\n",
    "print(f\"Composed prompt: {composed_prompt}\")\n",
    "\n",
    "response = llm.invoke(composed_prompt)\n",
    "print(f\"Response: {response.content}\")\n",
    "\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. KEY INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìã Key Information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ PromptTemplate: For simple string templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ ChatPromptTemplate: For structured conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ input_variables: Define template variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ .format(): Fill in template variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ .format_messages(): For chat templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Templates enable reusability and consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ Few-shot prompting improves response quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STUDENT TODO EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù STUDENT TODO EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTODO: Create a FewShotPromptTemplate for a sentiment analysis task\\n\\nRequirements:\\n1. Create at least 4 examples showing different sentiments (positive, negative, neutral, mixed)\\n2. Each example should have:\\n   - \"text\": The input text to analyze\\n   - \"sentiment\": The sentiment classification\\n   - \"confidence\": A confidence score (high/medium/low)\\n\\n3. Use an appropriate prefix that explains the task\\n4. Test your template with a new sentence\\n\\nExample structure to get you started:\\nsentiment_examples = [\\n    {\\n        \"text\": \"I love this product! It\\'s amazing!\",\\n        \"sentiment\": \"positive\",\\n        \"confidence\": \"high\"\\n    },\\n    # Add more examples here...\\n]\\n\\nHint: Think about edge cases like sarcasm or mixed feelings!\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "\n",
    "\"\"\"\n",
    "TODO: Create a FewShotPromptTemplate for a sentiment analysis task\n",
    "\n",
    "Requirements:\n",
    "1. Create at least 4 examples showing different sentiments (positive, negative, neutral, mixed)\n",
    "2. Each example should have:\n",
    "   - \"text\": The input text to analyze\n",
    "   - \"sentiment\": The sentiment classification\n",
    "   - \"confidence\": A confidence score (high/medium/low)\n",
    "\n",
    "3. Use an appropriate prefix that explains the task\n",
    "4. Test your template with a new sentence\n",
    "\n",
    "Example structure to get you started:\n",
    "sentiment_examples = [\n",
    "    {\n",
    "        \"text\": \"I love this product! It's amazing!\",\n",
    "        \"sentiment\": \"positive\",\n",
    "        \"confidence\": \"high\"\n",
    "    },\n",
    "    # Add more examples here...\n",
    "]\n",
    "\n",
    "Hint: Think about edge cases like sarcasm or mixed feelings!\n",
    "\"\"\"\n",
    "\n",
    "# Your code here:\n",
    "# sentiment_examples = [\n",
    "#     # TODO: Add your examples\n",
    "# ]\n",
    "\n",
    "# sentiment_example_template = PromptTemplate(\n",
    "#     # TODO: Define your template\n",
    "# )\n",
    "\n",
    "# sentiment_few_shot_prompt = FewShotPromptTemplate(\n",
    "#     # TODO: Configure your few-shot template\n",
    "# )\n",
    "\n",
    "# Test your template:\n",
    "# test_sentence = \"The movie was okay, not great but watchable\"\n",
    "# response = llm.invoke(sentiment_few_shot_prompt.format(text=test_sentence))\n",
    "# print(f\"Sentiment analysis: {response.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcp-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
