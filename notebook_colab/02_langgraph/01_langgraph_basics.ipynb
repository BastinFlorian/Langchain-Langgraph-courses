{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langgraph langchain-google-genai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Basics - Understanding StateGraph Fundamentals\n",
    "\n",
    "This script introduces the core concepts of LangGraph:\n",
    "- Creating a StateGraph with typed state\n",
    "- Defining nodes and their functions\n",
    "- Adding edges and conditional routing\n",
    "- Compiling and running the graph\n",
    "- Understanding state persistence across nodes\n",
    "\n",
    "Documentation: https://langchain-ai.github.io/langgraph/concepts/low_level/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LangGraph Basics: StateGraph Fundamentals ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Literal, TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    raise ValueError(\"Please set GOOGLE_API_KEY in your .env file\")\n",
    "\n",
    "print(\"=== LangGraph Basics: StateGraph Fundamentals ===\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Defining State Schema\n",
      "------------------------------\n",
      "✓ Created ChatState with messages, user_name, conversation_count, and mood\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"1. Defining State Schema\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    \"\"\"State schema for our chat graph\"\"\"\n",
    "    messages: list[str]\n",
    "    user_name: str\n",
    "    conversation_count: int\n",
    "    mood: Literal[\"happy\", \"neutral\", \"sad\"]\n",
    "\n",
    "\n",
    "print(\"✓ Created ChatState with messages, user_name, conversation_count, and mood\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Initialize LLM\n",
      "--------------------\n",
      "✓ Initialized Gemini 2.0 Flash model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"2. Initialize LLM\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"✓ Initialized Gemini 2.0 Flash model\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Node Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Define Node Functions\n",
      "-------------------------\n",
      "✓ Defined three functions: greet_user, generate_response, should_continue\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"3. Define Node Functions\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "\n",
    "def greet_user(state: ChatState) -> ChatState:\n",
    "    \"\"\"First node: Greet the user and analyze mood\"\"\"\n",
    "    print(f\"  → Greeting user: {state['user_name']}\")\n",
    "\n",
    "    # Analyze mood from the last message if available\n",
    "    if state[\"messages\"]:\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if any(word in last_message.lower() for word in [\"sad\", \"upset\", \"angry\", \"frustrated\"]):\n",
    "            mood = \"sad\"\n",
    "        elif any(word in last_message.lower() for word in [\"happy\", \"great\", \"awesome\", \"excited\"]):\n",
    "            mood = \"happy\"\n",
    "        else:\n",
    "            mood = \"neutral\"\n",
    "    else:\n",
    "        mood = \"neutral\"\n",
    "\n",
    "    greeting = f\"Hello {state['user_name']}! Nice to meet you.\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [greeting],\n",
    "        \"mood\": mood,\n",
    "        \"conversation_count\": state[\"conversation_count\"] + 1\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_response(state: ChatState) -> ChatState:\n",
    "    \"\"\"Second node: Generate AI response based on mood\"\"\"\n",
    "    print(f\"  → Generating response for mood: {state['mood']}\")\n",
    "\n",
    "    # Create context-aware prompt based on mood\n",
    "    if state[\"mood\"] == \"happy\":\n",
    "        system_prompt = \"You are an enthusiastic and cheerful assistant. Match the user's positive energy!\"\n",
    "    elif state[\"mood\"] == \"sad\":\n",
    "        system_prompt = \"You are a compassionate and supportive assistant. Be gentle and understanding.\"\n",
    "    else:\n",
    "        system_prompt = \"You are a helpful and professional assistant.\"\n",
    "\n",
    "    # Get the last user message (skip our greeting)\n",
    "    user_messages = [msg for msg in state[\"messages\"]\n",
    "                     if not msg.startswith(\"Hello\")]\n",
    "\n",
    "    if user_messages:\n",
    "        last_message = user_messages[-1]\n",
    "        response = llm.invoke(\n",
    "            f\"{system_prompt}\\n\\nUser message: {last_message}\")\n",
    "        ai_response = response.content\n",
    "    else:\n",
    "        ai_response = \"How can I help you today?\"\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"messages\": state[\"messages\"] + [ai_response],\n",
    "        \"conversation_count\": state[\"conversation_count\"] + 1\n",
    "    }\n",
    "\n",
    "\n",
    "def should_continue(state: ChatState) -> Literal[\"continue\", \"end\"]:\n",
    "    \"\"\"Conditional edge: Decide whether to continue conversation\"\"\"\n",
    "    # End after 4 exchanges to keep demo short\n",
    "    if state[\"conversation_count\"] >= 4:\n",
    "        print(\"  → Conversation limit reached, ending...\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"  → Continuing conversation...\")\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "print(\"✓ Defined three functions: greet_user, generate_response, should_continue\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Create StateGraph\n",
      "--------------------\n",
      "✓ Created StateGraph and added nodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"4. Create StateGraph\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Initialize the StateGraph with our schema\n",
    "workflow = StateGraph(ChatState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"greet\", greet_user)\n",
    "workflow.add_node(\"respond\", generate_response)\n",
    "\n",
    "print(\"✓ Created StateGraph and added nodes\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Add Edges\n",
      "---------------\n",
      "✓ Added edges: START → greet → respond\n",
      "✓ Added conditional edge: respond → (continue/end)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"5. Add Edges\")\n",
    "print(\"-\" * 15)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "workflow.add_edge(START, \"greet\")\n",
    "workflow.add_edge(\"greet\", \"respond\")\n",
    "\n",
    "# Add conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"respond\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"respond\",  # Loop back to respond\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ Added edges: START → greet → respond\")\n",
    "print(\"✓ Added conditional edge: respond → (continue/end)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compile Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Compile Graph\n",
      "------------------\n",
      "✓ Graph compiled successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"6. Compile Graph\")\n",
    "print(\"-\" * 18)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"✓ Graph compiled successfully!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. Run the Graph\n",
      "------------------\n",
      "Initial state:\n",
      "  User: Alice\n",
      "  Messages: 1\n",
      "  Count: 0\n",
      "\n",
      "Running graph...\n",
      "  → Greeting user: Alice\n",
      "  → Generating response for mood: happy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → Continuing conversation...\n",
      "  → Generating response for mood: happy\n",
      "  → Continuing conversation...\n",
      "  → Generating response for mood: happy\n",
      "  → Conversation limit reached, ending...\n",
      "\n",
      "Final result:\n",
      "  Total exchanges: 4\n",
      "  Final mood: happy\n",
      "  Messages exchanged: 5\n",
      "\n",
      "Conversation flow:\n",
      "  User: I'm feeling great today! Can you help me with Python?\n",
      "  Assistant: Hello Alice! Nice to meet you.\n",
      "  AI: That's FANTASTIC to hear! I'm feeling great too, knowing I get to help you with Python! Let's do thi...\n",
      "  Assistant: WOOHOO! 🎉 I'm absolutely buzzing with excitement to be working with YOU! Your enthusiasm is contagio...\n",
      "  AI: YAY! I'm buzzing right back at you! 🎉 I LOVE data wrangling challenges! Let's absolutely CRUSH those...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"7. Run the Graph\")\n",
    "print(\"-\" * 18)\n",
    "\n",
    "# Initial state\n",
    "initial_state = {\n",
    "    \"messages\": [\"I'm feeling great today! Can you help me with Python?\"],\n",
    "    \"user_name\": \"Alice\",\n",
    "    \"conversation_count\": 0,\n",
    "    \"mood\": \"neutral\"\n",
    "}\n",
    "\n",
    "print(\"Initial state:\")\n",
    "print(f\"  User: {initial_state['user_name']}\")\n",
    "print(f\"  Messages: {len(initial_state['messages'])}\")\n",
    "print(f\"  Count: {initial_state['conversation_count']}\")\n",
    "print()\n",
    "\n",
    "# Run the graph\n",
    "print(\"Running graph...\")\n",
    "result = app.invoke(initial_state)\n",
    "\n",
    "print(\"\\nFinal result:\")\n",
    "print(f\"  Total exchanges: {result['conversation_count']}\")\n",
    "print(f\"  Final mood: {result['mood']}\")\n",
    "print(f\"  Messages exchanged: {len(result['messages'])}\")\n",
    "print()\n",
    "\n",
    "print(\"Conversation flow:\")\n",
    "for i, message in enumerate(result[\"messages\"]):\n",
    "    speaker = \"User\" if i == 0 else (\"Assistant\" if i % 2 == 1 else \"AI\")\n",
    "    print(f\"  {speaker}: {message[:100]}{'...' if len(message) > 100 else ''}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Understanding State Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key concepts demonstrated:\n",
    "- • State is automatically passed between nodes\n",
    "- • Each node can read and modify the shared state  \n",
    "- • StateGraph ensures type safety with TypedDict\n",
    "- • Conditional edges enable dynamic routing\n",
    "- • The graph maintains state throughout execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Student Exercise - Simple Chatbot with Logic\n",
    "\n",
    "Create your own StateGraph that implements a simple customer support chatbot:\n",
    "\n",
    "1. Create a state schema with:\n",
    "   - user_query: str\n",
    "   - category: Literal[\"technical\", \"billing\", \"general\"]\n",
    "   - priority: Literal[\"low\", \"medium\", \"high\"]\n",
    "   - resolved: bool\n",
    "\n",
    "2. Create nodes for:\n",
    "   - classify_query: Analyze the query and set category/priority\n",
    "   - handle_technical: Handle technical issues\n",
    "   - handle_billing: Handle billing issues  \n",
    "   - handle_general: Handle general questions\n",
    "   - escalate: For high priority or unresolved issues\n",
    "\n",
    "3. Add conditional routing:\n",
    "   - From classify_query to appropriate handler based on category\n",
    "   - From handlers to escalate if high priority or unresolved\n",
    "   - Otherwise to END\n",
    "\n",
    "4. Test with different types of queries and observe the routing\n",
    "\n",
    "Hint: Use the user_query to determine category (keywords like \"password\", \"login\" = technical, \"payment\", \"refund\" = billing, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "# TODO: Implement the customer support chatbot StateGraph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
